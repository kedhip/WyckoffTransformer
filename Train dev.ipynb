{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:04:08.928601Z",
     "iopub.status.busy": "2024-05-24T08:04:08.928454Z",
     "iopub.status.idle": "2024-05-24T08:04:08.934949Z",
     "shell.execute_reply": "2024-05-24T08:04:08.934548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=Train dev.ipynb\n",
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NOTEBOOK_NAME Train dev.ipynb\n",
    "%env CUDA_DEVICE_ORDER PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES 0\n",
    "%env PYTORCH_CUDA_ALLOC_CONF backend:cudaMallocAsync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:04:08.964226Z",
     "iopub.status.busy": "2024-05-24T08:04:08.963815Z",
     "iopub.status.idle": "2024-05-24T08:04:09.796178Z",
     "shell.execute_reply": "2024-05-24T08:04:09.795408Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:04:09.798691Z",
     "iopub.status.busy": "2024-05-24T08:04:09.798535Z",
     "iopub.status.idle": "2024-05-24T08:04:13.923437Z",
     "shell.execute_reply": "2024-05-24T08:04:13.922748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 7 8 9\n"
     ]
    }
   ],
   "source": [
    "from mp_20_utils import load_all_data\n",
    "device = 'cuda'\n",
    "dataset = 'mp_20_biternary'\n",
    "datasets_pd, torch_datasets, site_to_ids, element_to_ids, spacegroup_to_ids, max_len, max_enumeration, enumeration_stop, enumeration_pad = load_all_data(\n",
    "    dataset=dataset)\n",
    "print(max_len, max_enumeration, enumeration_stop, enumeration_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:04:13.926201Z",
     "iopub.status.busy": "2024-05-24T08:04:13.925945Z",
     "iopub.status.idle": "2024-05-24T08:04:14.308127Z",
     "shell.execute_reply": "2024-05-24T08:04:14.307621Z"
    }
   },
   "outputs": [],
   "source": [
    "from cascade_transformer.model import CascadeTransformer\n",
    "from wyckoff_transformer import WyckoffTrainer\n",
    "from tokenization import PAD_TOKEN, MASK_TOKEN\n",
    "n_space_groups = len(spacegroup_to_ids)\n",
    "# Not all 230 space groups are present in the data\n",
    "# Embedding doesn't support uint8. Sad!\n",
    "dtype = torch.int64\n",
    "cascade_order = (\"elements\", \"symmetry_sites\", \"symmetry_sites_enumeration\")\n",
    "# (N_i, d_i, pad_i)\n",
    "assert max_enumeration + 1 == enumeration_stop\n",
    "assert max_enumeration + 2 == enumeration_pad\n",
    "enumeration_mask = max_enumeration + 3\n",
    "assert enumeration_mask < torch.iinfo(dtype).max\n",
    "\n",
    "cascade = (\n",
    "    (len(element_to_ids), 64, torch.tensor(element_to_ids[PAD_TOKEN], dtype=dtype, device=device)),\n",
    "    (len(site_to_ids), 64 - 1, torch.tensor(site_to_ids[PAD_TOKEN], dtype=dtype, device=device)),\n",
    "    (enumeration_mask + 1, None, torch.tensor(enumeration_pad, dtype=dtype, device=device))\n",
    ")\n",
    "model = CascadeTransformer(\n",
    "    n_start=n_space_groups,\n",
    "    cascade=cascade,\n",
    "    n_head=4,\n",
    "    d_hid=256,\n",
    "    n_layers=4,\n",
    "    dropout=0.1,\n",
    "    use_mixer=True).to(device)\n",
    "# Our dynamic discard of predicting PAD calls for frequent recompilation\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T08:04:14.312127Z",
     "iopub.status.busy": "2024-05-24T08:04:14.311264Z",
     "iopub.status.idle": "2024-05-24T11:17:16.052200Z",
     "shell.execute_reply": "2024-05-24T11:17:16.051290Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkazeev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/kna/WyckoffTransformer/wandb/run-20240524_160417-eya1llbi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgraceful-wave-239\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer/runs/eya1llbi\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 val_loss_epoch 110.37109375 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kna/WyckoffTransformer/pytorch/torch/optim/lr_scheduler.py:1300: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 val_loss_epoch 51.28900909423828 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120 val_loss_epoch 49.35118865966797 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 val_loss_epoch 46.471858978271484 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 760 val_loss_epoch 45.8333740234375 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1080 val_loss_epoch 44.10159683227539 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1200 val_loss_epoch 42.67142868041992 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1220 val_loss_epoch 41.50479507446289 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1320 val_loss_epoch 40.442752838134766 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2580 val_loss_epoch 40.442298889160156 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6600 val_loss_epoch 40.22306823730469 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6640 val_loss_epoch 40.21381759643555 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6660 val_loss_epoch 39.729244232177734 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6920 val_loss_epoch 39.422401428222656 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6980 val_loss_epoch 39.25120544433594 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7000 val_loss_epoch 39.17072677612305 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7080 val_loss_epoch 39.04907989501953 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7100 val_loss_epoch 38.69624710083008 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7180 val_loss_epoch 38.376766204833984 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8400 val_loss_epoch 38.33898162841797 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8440 val_loss_epoch 37.9315185546875 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8580 val_loss_epoch 37.91562271118164 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8620 val_loss_epoch 37.639583587646484 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8780 val_loss_epoch 37.156009674072266 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9500 val_loss_epoch 36.99886703491211 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11340 val_loss_epoch 36.71635437011719 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11580 val_loss_epoch 36.5906982421875 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11660 val_loss_epoch 36.52017593383789 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11680 val_loss_epoch 36.41949462890625 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11740 val_loss_epoch 36.40169143676758 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11780 val_loss_epoch 35.936771392822266 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13620 val_loss_epoch 35.8924674987793 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14180 val_loss_epoch 35.81734848022461 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14600 val_loss_epoch 35.8140983581543 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15120 val_loss_epoch 35.790870666503906 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15300 val_loss_epoch 35.759098052978516 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15400 val_loss_epoch 35.68293380737305 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15800 val_loss_epoch 35.66883087158203 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16280 val_loss_epoch 35.59356689453125 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16340 val_loss_epoch 35.57943344116211 saved to checkpoints/2024-05-24_16-04-14/best_model_params.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 1.831 MB of 1.831 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 1.831 MB of 1.839 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 1.202 MB of 2.842 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 2.842 MB of 2.842 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 2.842 MB of 2.842 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 2.842 MB of 2.842 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: known_cascade_len â–ˆâ–ˆâ–…â–…â–â–ˆâ–â–ˆâ–…â–â–ˆâ–ˆâ–ˆâ–ˆâ–…â–â–ˆâ–…â–…â–…â–…â–â–ˆâ–…â–ˆâ–ˆâ–â–â–…â–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     known_seq_len â–ˆâ–ƒâ–‚â–ˆâ–…â–‡â–‡â–„â–ƒâ–†â–‚â–„â–†â–‚â–‚â–‡â–†â–‡â–ƒâ–†â–„â–†â–â–‚â–‚â–…â–†â–â–‡â–‡â–ƒâ–†â–ƒâ–â–‚â–‡â–ˆâ–†â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                lr â–ˆâ–‡â–…â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_batch â–â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–…â–â–â–â–â–â–â–â–‚â–…â–â–â–â–ˆâ–â–â–â–â–â–‚â–‚â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_epoch â–‚â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_loss_epoch â–‚â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             epoch 40000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: known_cascade_len 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     known_seq_len 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_batch 1099.88013\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_epoch 35.75783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_loss_epoch 35.65189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mgraceful-wave-239\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer/runs/eya1llbi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/kazeev/WyckoffTransformer\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240524_160417-eya1llbi/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "pad_dict = {\n",
    "    \"elements\": element_to_ids[PAD_TOKEN],\n",
    "    \"symmetry_sites\": site_to_ids[PAD_TOKEN],\n",
    "    \"symmetry_sites_enumeration\": enumeration_pad\n",
    "}\n",
    "mask_dict = {\n",
    "    \"elements\": element_to_ids[MASK_TOKEN],\n",
    "    \"symmetry_sites\": site_to_ids[MASK_TOKEN],\n",
    "    \"symmetry_sites_enumeration\": enumeration_mask\n",
    "}\n",
    "trainer = WyckoffTrainer(\n",
    "    model, torch_datasets, pad_dict, mask_dict, cascade_order, \"spacegroup_number\", max_len, device, dtype=dtype)\n",
    "trainer.train(epochs=40000, val_period=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyckofftransformer-FeCwefly-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
